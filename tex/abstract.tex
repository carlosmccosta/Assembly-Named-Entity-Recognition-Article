\begin{abstract}

Teaching industrial robots by demonstration can significantly decrease the re-purposing costs of assembly lines worldwide. To achieve this goal, the robot needs to semantically detect and track each component with high accuracy. To speedup the initial object recognition phase, the learning system can gather information from assembly manuals in order to identify which parts are required for assembling the new product (avoiding exhaustive search in a large model database) and if possible also extract the assembly order and spatial relation between them along with the tools needed. This paper presents a detailed analysis of the fine tunning of the Stanford \gls{ner} system for this text tagging task. Starting from the recommended configuration, it was performed 91 tests targeting the main features / parameters. Each test only changed a single parameter in relation to the recommend configuration, and its goal was see the impact of the new configuration in the precision, recall, accuracy and F1 metrics. This analysis allowed to fine tune the Stanford \gls{ner} system, achieving an improvement of 3\% in F1, 5\% in recall and 1\% in precision. These results were retrieved in our manually annotated dataset containing assembly operations text for alternators, gearboxes and engines, which were written in a language discourse that ranges from professional to informal. The dataset can also be used to evaluate other information extraction and computer vision systems, since most assembly operations have pictures and diagrams showing the necessary product parts, their assembly order and relative spacial disposition.

\end{abstract}
