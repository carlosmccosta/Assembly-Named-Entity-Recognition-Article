\section{Conclusions}\label{sec:conclusions}

This paper presented a detailed analysis of the fine tuning of the Stanford \gls{ner} system in our annotated corpus with assembly operations. It was performed 91 tests targeting the main configurations / features of the Stanford \gls{ner} implementation, in which the recommended configuration was used as a starting point and then each test either changed a numeric parameter or enabled / disabled a given feature. This allowed to select the best training configuration for the statistical language models (\glspl{crf}), which achieved 85.91\% in precision, 83.51\% in recall and 84.69\% in F1 (corresponding to an improvement of 3.23\% in F1, 5.79\% in recall and 0.35\% in precision over the recommended configuration given in the official documentation). Although these tests were performed with our target corpus, we expect that the parameters / features which either significantly improved or degraded the \gls{ner} recognition performance will be transversal to the dataset used, allowing other researchers to use this configuration as a starting point for their specific corpus.

Our dataset contains assembly operations of alternators, gearboxes and engines in textual form and is complemented with object pictures and assembly diagrams. For evaluating \gls{ner} systems using this dataset, each assembly operation has associated a list with the parts and quantities needed to successfully perform the product assembly. In order to have a representative dataset, it is provided assembly operations written in a professional / structured manner and also in an informal and colloquial language register. Moreover, a small yet representative part is manually annotated at the token level in a \gls{tsv} format.

This dataset was built for evaluating \gls{ner} systems, but can also be used to evaluate information extraction and computer vision systems, given the large textual and image information that it provides for each assembly operation.
