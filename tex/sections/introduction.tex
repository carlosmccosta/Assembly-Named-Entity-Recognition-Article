\section{Introduction}\label{sec:introduction}

Programming of industrial robots for assembly operations is a meticulous and arduous task that requires a significant engineering effort with long testing and deployment phases. For high volume manufacturing this cost is acceptable, but it is too expensive to repurpose robots for small volume production using traditional programming approaches. These issues can be overcome with robots that can learn new assembly skills by observing experienced operators and interacting with them through natural language. To achieve these goals, the robot needs to successfully recognize the objects within its workspace and semantically track their pose with high precision while the operator demonstrates how to perform the assembly operations. Moreover, it must be able to understand any instructions that the operator might give and also have the ability to recall them if asked later on. This type of teaching allows rapid reprogramming of flexible robotic assembly cells for new tasks, but it can be speed up even further if there are assembly manuals available, which allows the robotic system to extract the objects and their assembly spacial disposition from the textual and visual representations. By knowing which objects to expect for a given teaching session, the object recognition system efficiency can be significantly increased (by limiting the object search database). Moreover, this preliminary information extraction phase reduces the human teaching phase to only the operations that lack detailed information. This type of information extraction problem is know in the \gls{nlp} domain as \gls{ner}, and is usually tackled with \gls{ml} approaches that rely on statistical models such as \glspl{crf} or \glspl{hmm}, coupled with fine tuned regular expression matching systems and gazetteers. One of the most used \gls{ner} implementation for this task is the Stanford \gls{ner} \cite{Finkel2005}, which is integrated into the well known Stanford CoreNLP toolkit \cite{manning2014}.

This paper provides a detailed analysis of the impact that each of the main features available in the Stanford \gls{ner} system have in the overall entity recognition performance, allowing to fine tune the language model training to a given corpus. Each of the 92 tests starts with the recommended configuration and then changes a single parameter or enables / disables a given feature. This allows to identify which features should be used in order to obtain the optimal model training configuration. Although these tests were perform with our target corpus, we expect that the features which either significantly improve or degrade the recognition performance will be transversal to the corpus used. In our annotated dataset of assembly operations, this analysis allowed the fine tuning of the Stanford \gls{ner} system which achieved an improvement of 3\% in F1, 5\% in recall and 1\% in precision over the recommended configuration given in the official documentation. Our annotated dataset contained assembly instructions of alternators, gearboxes and engines in several writing styles, from highly professional and structured text to colloquial and informal language. These assembly operations were extracted from \gls{pdf} files that besides textual descriptions also had assembly pictures and diagrams. As such, this dataset can be used for evaluating systems that combine both natural language processing algorithms and also computer vision and information extraction systems. Besides token level manual annotation, each assembly operation has a list with the required parts for successfully performing the product assembly. For speeding up testing, the dataset is already split into 80\% of training text and 20\% of testing text.

In the following section it will be given a brief overview of applications of \gls{nlp} in robotics and also the main related work on extraction of assembly information from textual representations. \Cref{sec:dataset-sources} describes the main dataset sources for the 3 product types with assembly operations. \Cref{sec:dataset-preparation} presents the main steps that were performed to extract and clean the text from the \glspl{pdf}.

%todo

Finally, \cref{sec:conclusions} presents the conclusions and \cref{sec:future-work} gives an overview of possible future work.
