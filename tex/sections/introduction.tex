\section{Introduction}\label{sec:introduction}

Programming of industrial robots for assembly operations is a meticulous and arduous task that requires a significant engineering effort with long testing and deployment phases. For high volume manufacturing this cost is acceptable, but it is too expensive to repurpose robots for small volume production using traditional programming approaches. These issues can be overcome with robots that can learn new assembly skills by observing experienced operators and interacting with them through natural language. To achieve these goals, the robot needs to successfully recognize the objects within its workspace and semantically track their pose with high precision while the operator demonstrates how to perform the assembly operations. Moreover, it must be able to understand any instructions that the operator might give and also have the ability to recall them if asked later on. This type of teaching allows rapid reprogramming of flexible robotic assembly cells for new tasks, but it can be speed up even further if there are assembly manuals available, which allows the robotic system to extract the objects and their assembly spatial disposition from the textual and visual representations. By knowing which objects to expect for a given teaching session, the object recognition system efficiency can be significantly increased (by limiting the object search database). Moreover, this preliminary learning phase reduces the human teaching to only the operations that lack detailed information. This type of information extraction problem is know in the \gls{nlp} domain as \gls{ner}, and is usually tackled with \gls{ml} approaches that rely on statistical models such as \glspl{crf} or \glspl{hmm}, coupled with fine tuned regular expression matching systems and gazetteers. One of the most used \gls{ner} implementation for this task is the Stanford \gls{ner}\footnote{\url{https://nlp.stanford.edu/software/CRF-NER.shtml}} \cite{Finkel2005}, which is integrated into the well known Stanford CoreNLP toolkit \cite{manning2014}.

This paper provides a detailed analysis of the impact that each of the main features available in the Stanford \gls{ner} system have in the overall entity recognition performance, allowing to fine tune the language model training to a given corpus. The proper selection of the \gls{crf} training features for a given application domain can have a significant effect on the entity recognition performance \cite{Tkachenko2012}. For example, simple token level training of \glspl{crf} leads to poor performance, but if text features such as word prefix / suffix / shape, orthographic / morphological clues, along with word / phrasal clustering and \gls{pos} tags are used, this can result in a \gls{crf} language model that can be very effective for recognizing the intended entities. Moreover, the performance can be improved even further if gazetteers and external knowledge databases are used.

It was performed 91 tests that started with the recommended configuration and then either changed a single parameter or enabled / disabled a given feature. This allowed to identify which features should be used in order to obtain the optimal model training configuration. Although these tests were performed with our target corpus, we expect that the features / parameters which either significantly improved or degraded the recognition performance will be transversal to the corpus used. In our new annotated dataset of assembly operations this analysis allowed to fine tune the Stanford \gls{ner} system, which managed to achieve a precision of 89.91\%, recall of 83.51\% and F1 of 84.69\%, corresponding to an improvement of 3.23\% in F1, 5.79\% in recall and 0.35\% in precision over the recommended configuration given in the official documentation. Our annotated dataset contained assembly instructions of alternators, gearboxes and engines in several writing styles, from highly professional and structured text to colloquial and informal language. These assembly operations were extracted from \gls{pdf} files that besides textual descriptions also had assembly pictures and diagrams. As such, this dataset can be used for evaluating systems that combine both natural language processing algorithms and also computer vision and information extraction systems. Besides token level manual annotation, each assembly operation has a list with the required parts for successfully performing the product assembly. For speeding up testing, the dataset is already split into 84\% of training text and 16\% of testing text.

In the following section it will be given a brief overview of applications of \gls{nlp} in robotics and also the main related work on extraction of assembly information from textual representations. \Cref{sec:dataset-sources} describes the main dataset sources for the 3 product types with assembly operations. \Cref{sec:dataset-preparation} presents the main steps that were performed to extract and clean the text from the \glspl{pdf}. \Cref{sec:model-tuning-results} gives a brief analysis of the fine tuning results, informing what were the features / parameters that either significantly increased or decreased the recognition performance. Finally, \cref{sec:conclusions} presents the conclusions and \cref{sec:future-work} gives an overview of possible future work.
