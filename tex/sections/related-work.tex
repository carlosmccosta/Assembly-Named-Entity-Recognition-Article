\section{Related work}\label{sec:related-work}

\gls{nlp} algorithms have been integrated into robotics systems for a myriad of applications, ranging from control of industrial robotic arms \cite{Akan2011,Watanabe2006} and mobile robots \cite{Matuszek2013} to complex interaction with humanoid systems using a combination of voice, text and image perception analysis \cite{Neo2008,Barabas2012,Antunes2016}. For the voice and textual teaching, the objects names and relations can be identified using \gls{ner} algorithms \cite{Leon2014,Dlugolinsky2013}. This type of approach usually relies on syntactic and semantic parsing of the text and also in machine learning algorithms \cite{Ekbal2012} (such as \glspl{svm}, \glspl{hmm} and \glspl{crf}) in order to be able to recognize previously unseen object names. It may present some challenges \cite{Ratinov2009}, but this methodology can achieve multilingual entity recognition \cite{Rami2014} if language agnostic attributes are used. Other complementary techniques, such as gazetteers, can improve overall recognition by providing a list of known entities that can be used for exact / partial string matching. This might be a suitable approach when we have extensive and representative entity lists and we are not expecting the text we are going to annotate to have significant new entities. Otherwise, the gazetteers might increase the number of false negatives for unseen entities. This problem might be overcome \cite{Smith2006} by normalizing the gazetteer features and using two \gls{crf} models for text tagging (one trained with the gazetteer features and another without), that are later on combined with a logarithmic opinion pool. This is similar to a mixture model, but uses a weighted multiplicative combination of models instead of a weighted additive combination.

Advanced applications of \gls{nlp} algorithms include the teaching of assembly operations to robot arm manipulation systems by human operators. The JAST robot \cite{Rickert2007} was implemented using a multi-agent system capable of learning assembly operations by interpreting human voice commands along with their gestures and gaze. The speech recognition system used a \gls{ccg} and a semantics module to analyze if the operator was making statements for teaching, asking for information or giving answers to previous questions made by the robot. The vision system besides tracking the hands and gaze of the operator to perform a better speech analysis, it also recognized the assembly objects within the robot workspace using template matching techniques.

Industrial applications of \gls{nlp} \cite{Stenmark2013} can also rely on multi-lingual statistical semantic parsers to extract assembly operations from natural language sentences given by remote operators. The system developed in the ROSETTA\footnote{\url{http://www.fp7rosetta.org}} research project used a client-server architecture containing a natural language parser, a \gls{kif} and an engineering system. The parser main goal was to find predicates with their respective arguments in sentences and establish coreference chains. The \gls{kif} contained ontologies and semantically annotated skills that were used for filtering the predicates and return only the ones relevant for assembly operations. Lastly, the engineering system was a high level programming interface that used the predicates found by the parser to select the appropriate skills for assembly while also matching the predicates arguments with the knowledge database in order to identify the objects and analyze in which branch of the assembly tree they should be inserted for achieving proper assembly order.

Besides voice and textual input from humans, the assembly information can also be retrieved from online web pages or knowledge repositories. During the RoboHow\footnote{\url{http://www.robohow.eu}} research project it was developed a system \cite{Tenorth2010} that could extract the assembly graph by performing a syntactic and semantic analysis using a \gls{pcfg} parser and a \gls{pos} tagger followed by word sense retrieval and disambiguation using the WordNet database and the Cyc ontology. After having a preliminary assembly plan, it was executed in simulation in order to perform a high level validation and allow the optimization of the robot movements. If ambiguous or missing information was detected, the system tried to generate a valid assembly plan by analyzing the objects' environment, assembly context and also similar operations stored in its knowledge database.

Named entity recognition can also be useful to identify key information from mission operation orders given to operators of robotics systems, such as \glspl{uav}. Highlighting entities such as persons, times, locations, coordinates, targets and organizations allows the human operators to extract the necessary mission information faster. Moreover, in the future it might even be possible to have the robotic system autonomously extract all the required information to carry on the mission without human assistance. This task can be performed using a machine learning approach \cite{Chesworth2016} that relies on \gls{crf} statistical models that use features such as word lists, regular expressions, prefixes / suffixes, word case and also unigram / bigram / trigrams models. The evaluation of the \gls{ner} system was performed using metrics such as precision, recall and accuracy and used 9-fold cross validation for having a rotating train / test dataset in order to avoid model over-fitting.

Several \gls{ner} systems and datasets have been presented over the years for news and tweets \cite{Dojchinovski2013}. This paper aims to provide an evaluation of the Stanford \gls{ner} system within the assembly operations domain using our new manually annotated corpus containing a diverse range of assembly manuals for small complex objects (alternators, gearboxes and engines) written in a language discourse that ranges from professional to informal.
